{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Gaussian Process (DGP)"
      ],
      "metadata": {
        "id": "SCv8IKp-JLrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg0rZyg-JI4R"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title install packages\n",
        "!pip install gpytorch\n",
        "!pip install optuna\n",
        "!pip install watermark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import packages\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import gpytorch\n",
        "from torch.nn import Linear\n",
        "from gpytorch.means import ConstantMean, LinearMean\n",
        "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
        "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
        "from gpytorch.distributions import MultivariateNormal\n",
        "from gpytorch.models import ApproximateGP, GP\n",
        "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
        "from gpytorch.likelihoods import GaussianLikelihood\n",
        "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
        "from gpytorch.mlls import DeepApproximateMLL\n",
        "\n",
        "sns.reset_defaults()\n",
        "sns.set_context(context='talk', font_scale=1.0)\n",
        "cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "OiOPhjFNJU8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seed(seed):\n",
        "    # random\n",
        "    # random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "X_nQ-dITJY6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00265/CASP.csv')\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9_muHvzfJbCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = df.iloc[:, 1:], df.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# 説明変数、観測変数の標準化\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 参考文献では元のスケールにおけるRMSEを算出しているため、統計量を求めておく。\n",
        "m, s = y_train.mean(), y_train.std(ddof=0) #ddof : Delta degree of freedom, ddof=0: entire data is included->divided by n, ddof=1:samples from entire data->divided by (n-1)\n",
        "y_train = (y_train.values - m) / s\n",
        "y_test = (y_test.values - m) / s\n",
        "\n",
        "dtype = torch.float32\n",
        "X_train, X_test, y_train, y_test = (\n",
        "    torch.tensor(X_train, dtype=dtype),\n",
        "    torch.tensor(X_test, dtype=dtype),\n",
        "    torch.tensor(y_train, dtype=dtype),\n",
        "    torch.tensor(y_test, dtype=dtype)\n",
        "    )\n",
        "\n",
        "# データをGPUに配置\n",
        "if torch.cuda.is_available():\n",
        "    (X_train, X_test, y_train, y_test) = (\n",
        "        X_train.cuda(), X_test.cuda(), y_train.cuda(), y_test.cuda()\n",
        "    )\n",
        "\n",
        "# ミニバッチを読み込むためのDataLoaderを作成\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "5CKJZsm2Jizu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "tGj0VnOAKl8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepGPHiddenLayer(DeepGPLayer):\n",
        "  def __init__(self,input_dims,output_dims,num_inducing=128,mean_type=\"constant\"):\n",
        "    if output_dims is None:\n",
        "      #set initial points\n",
        "      inducing_points  = torch.randn(num_inducing,input_dims) #(num_inducing,input_dims)\n",
        "      batch_shape=torch.Size([])\n",
        "    else:\n",
        "      inducing_points = torch.randn(output_dims,num_inducing,input_dims)\n",
        "      batch_shape=torch.Size([output_dims])\n",
        "\n",
        "    #set approximate posterior distribution\n",
        "    variational_distribution = CholeskyVariationalDistribution(\n",
        "        num_inducing_points=num_inducing,\n",
        "        batch_sahpe=batch_shape\n",
        "    )\n",
        "\n",
        "    variational_strategy = VariationalStrategy(\n",
        "        self,\n",
        "        inducing_points,\n",
        "        variational_distribution,\n",
        "        learn_inducing_locations=True #position of inducing points is set trainable\n",
        "    )\n",
        "\n",
        "    super(DeepGPHiddenLayer,self).__init__(variational_strategy,input_dims,output_dims)\n",
        "\n",
        "    #mean function\n",
        "    if mean_type==\"constant\":\n",
        "      self.mean_module=ConstantMean(batch_shape=batch_shape)\n",
        "    else:\n",
        "      self.mean_module=LinearMean(input_dims)\n",
        "    self.covar_module = ScaleKernel(\n",
        "        #RBF kernel\n",
        "        RBFKernel(batch_shape=batch_shape,ard_num_dims=input_dims),\n",
        "        #\"\"\"ARD : Automatic Relevance Determination\n",
        "        #ARD allows different input dimensions/features to have different length scales or relevance. ard_num_dims typically specifies the number of input dimensions for which ARD will be applied, and input_dims likely provides this number.\n",
        "        #\"\"\"\n",
        "        batch_shape=batch_shape,\n",
        "        ard_num_dims=None\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return MultivariateNormal(mean_x,covar_x)\n",
        ""
      ],
      "metadata": {
        "id": "GE9Kl9kGJjqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DGP(DeepGP):\n",
        "  def __init__(self,input_dim,hidden_dim):\n",
        "    hidden_layer = DeepGPHiddenLayer(\n",
        "        input_dims=input_dim,\n",
        "        output_dims=hidden_dim,\n",
        "        #use linear mean function\n",
        "        mean_type=\"linear\",\n",
        "    )\n",
        "\n",
        "    last_layer = DeepGPHiddenLayer(\n",
        "        input_dims=hidden_layer.output_dims,\n",
        "        output_dims=None,\n",
        "        mean_type=\"constant\",\n",
        "    )\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_layer = hidden_layer\n",
        "    self.last_layer = last_layer\n",
        "    self.likelihood = GaussianLikelihood()\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    hidden_rep1 = self.hidden_layer(inputs)\n",
        "    output =  self.last_layer(hidden_rep1)\n",
        "    return output\n",
        "\n",
        "  def predict(self,test_loader):\n",
        "    with torch.no_grad(): #suppress gradient calculation\n",
        "      mus = [] #mu\n",
        "      variances = [] #covar\n",
        "      lls = [] #log likelihood\n",
        "      for x_batch,y_batch in test_loader:\n",
        "        preds = self.likelihood(self(x_batch))\n",
        "        mus.append(preds.mean)\n",
        "        variances.append(preds.variance)\n",
        "        lls.append(self.likelihood.log_marginal(y_batch,self(x_batch)))\n",
        "\n",
        "    return torch.cat(mus,dim=-1), torch.cat(variances,dim=-1),torch.cat(lls,dim=-1)\n",
        "\n",
        "input_dim = hidden_dim = X_train.shape[-1]\n",
        "model = DGP(input_dim,hidden_dim)\n",
        "if torch.cuda.is_available():\n",
        "  model=model.cuda()"
      ],
      "metadata": {
        "id": "oKoYqJphONYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Variatinal inference\n",
        "- approximate ELBO with sampling"
      ],
      "metadata": {
        "id": "5sV4Or2ZQ5n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 200\n",
        "num_samples = 10\n",
        "\n",
        "optimizer = torch.optim.Adam([{\"params\":model.parameters()}],lr=0.01)\n",
        "#objective fn\n",
        "mll = DeepApproximateMLL(VariationalELBO(model.likelihood,model,X_train.shape[-2]))\n",
        "\n",
        "losses = []\n",
        "epochs_iter = tqdm.notebook.tqdm(range(num_epochs),desc=\"Epoch\")\n",
        "for i in epochs_iter:\n",
        "  epoch_loss = []\n",
        "  for x_batch,y_batch in train_loader:\n",
        "    with gpytorch.settings.num_likelihood_samples(num_samples):\n",
        "      optimizer.zero_grad()\n",
        "      output=model(x_batch)\n",
        "      loss=-mll(output,y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss.append(loss.item())\n",
        "  losses.append(np.mean(epoch_loss))"
      ],
      "metadata": {
        "id": "2fYkD93LOD20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## predict"
      ],
      "metadata": {
        "id": "ibAqXyPyU4hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "#mean,varicance,log likelihood\n",
        "predictive_means,predictive_variances,test_lls = model.predict(test_loader)\n",
        "\n",
        "mse = mean_squared_error(y_test.cpu(),predictive_means.mean(0).cpu())*s**2\n",
        "\n",
        "print(f\"RMSE(DGP):{mse**0.5:.2f}\")\n",
        "print(f\"Log Likelihood(DGP) :{test_lls.mean().item():.2f} \")"
      ],
      "metadata": {
        "id": "0fcp7tK8SR2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lls"
      ],
      "metadata": {
        "id": "9QxfImLpVZO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}